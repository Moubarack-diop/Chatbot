# Chatbot

## Aper√ßu du projet
Notre projet consiste √† concevoir un chatbot destin√© √† assister le service client du Groupe Orange S√©n√©gal. Ce chatbot est bas√© sur l'Intelligence Artificielle et utilise dans sa g√©n√©ration de r√©ponse, un grand mod√®le de langage naturel (LLM). Dans notre cas nous avons utilis√© le Llama2 13B Chat, un mod√®le de langage d√©velopp√© par Meta AI, disponible via l'interface de Hugging Face Transformers.
Cependant,bien que les LLM soient puissants et capables de g√©n√©rer du contenu cr√©atif, ils peuvent produire des informations obsol√®tes ou incorrectes car ils sont form√©s sur des donn√©es statiques. Pour surmonter cette limitation, les syst√®mes de g√©n√©ration augment√©e de r√©cup√©ration (RAG) peuvent √™tre utilis√©s pour connecter notre LLM √† des donn√©es externes et obtenir des r√©ponses plus fiables. 
Nous avons alors r√©cup√©r√© les donn√©es FAQs contenues dans le site de [Orange Assistance](https://assistance.orange.sn/) grace au web scraping qu'on a directement ing√©r√© dans le LLM grace au RAG.
## C'est quoi le RAG

Les LLM sont form√©s sur un corpus de donn√©es volumineux mais fixe, ce qui limite leur capacit√© √† raisonner sur des informations priv√©es ou r√©centes. Le fine tuning est un moyen d'att√©nuer ce probl√®me, mais il n'est souvent pas bien adapt√© au rappel factuel et peut √™tre co√ªteux. La g√©n√©ration augment√©e de r√©cup√©ration (RAG) est devenue un m√©canisme populaire et puissant pour √©largir la base de connaissances d'un LLM, en utilisant des documents r√©cup√©r√©s √† partir d'une source de donn√©es externe pour fonder la g√©n√©ration de LLM via l'injection de contexte.

![Workflow_rag (1)](https://github.com/user-attachments/assets/d8179e97-5b5f-4c6c-9688-0aee3b5eff3a)

L'architecture ci-dessus est une excellente introduction aux bases de l'injection de contexte. Nous allons r√©sumer le processus d'injection de contexte comme suit :
1. D'abord nous collectons d'abord les donn√©es extraites du site de Orange Assistance grace au web scraping. Ces derni√®res sont ensuite transform√©es en fichier CSV.
   
2. Ensuite, les donn√©es sont charg√©es (text loader) puis segment√©es (text splitting). Les segments (tokens) sont essentiellement une courte cha√Æne de caract√®res, g√©n√©ralement de 4 caract√®res. Par exemple, le mot ¬´ g√©n√©ratif ¬ª peut √™tre divis√© en segments comme ¬´ ge ¬ª, ¬´ n ¬ª, ¬´ erat ¬ª et ¬´ ive ¬ª. Le LLM traite les donn√©es sous forme de segments.
 
3. Les tokens seront introduits dans un mod√®le d'embedding qui convertit les tokens en vecteurs. L'embeddings sont la fa√ßon dont les mots et les phrases sont repr√©sent√©s dans un espace vectoriel. Il existe plusieurs mod√®les d'embedding dans le march√© dans notre cas nous avons choisi celui de Hugging Face.
  
4. Les vecteurs produits √† partir du mod√®le d'embedding sont stock√©s dans des bases de donn√©es vectorielles. Dans notre cas, nous utiliserons ChromaDB.
   
5. Passons maintenant √† la partie int√©ressante. Lorsqu'un utilisateur pose une question, nous convertissons sa requ√™te en vecteur et recherchons les vecteurs les plus proches dans la base de donn√©es. Essentiellement, ce processus localise les fragments de texte les plus pertinents pour la requ√™te de l'utilisateur et les reconvertit en texte.
   
6. La question de l'utilisateur et les fragments de texte pertinents (contexte) seront inclus dans un prompt qui sera fourni au LLM. Sans aucune modification du LLM d'origine, le mod√®le peut fournir des r√©ponses impressionnantes √† la requ√™te en utilisant le contexte inject√©.

7. La r√©ponse g√©n√©r√©e et la question pr√©c√©dente de l'utilisateur seront introduites dans le prompt pour servir d'historique des conversations. Le LLM pourra s'en inspir√© lorsque l'utilisateur pose une autre question en rapport avec les question et r√©ponse pr√©c√©dentes.

## Technologies utilis√©es

### ü¶úÔ∏èüîó Langchain
LangChain est un framework open-source con√ßu pour faciliter le d√©veloppement d'applications bas√©es sur des mod√®les de langage (LLMs). Il fournit des ouils pour les pipelines de traitement de langage, g√®re les conversations longues en g√©rant l'historique contextuel. Il facilite l'int√©gration des bases de connaissances de meme que la conception des prompts.

### ü§ó Hugging Face
Hugging Face est une plateforme qui offre la posibilit√© d'acc√©der √† des mod√®les de langage pr√©entrain√©s Open source. Pour le cadre de notre projet, nous y avons t√©l√©charg√© le mod√®le Llama-2-13b-chat-hf. C'est un mod√®le gratuit fourni par Meta AI. N√©anmoins il existe d'autres LLM performants sur le march√© tels que GPT, Gemini,...; leur utilisation √©tant payante, nous avons pr√©f√©r√© travailler avec des LLM gratuits.

### ChromaDB
ChromaDB est une base de donn√©es vectorielle sp√©cialement con√ßue pour stocker et rechercher des donn√©es vectorielles. Elle va nous permettre de g√©rer nos donn√©es vectoris√©es Ces vecteurs seront utilis√©s pour effectuer des recherches bas√©es sur la similarit√©.

## Installation
Pour le d√©veloppement de notre Chatbot nous avons utilis√© Google Colab. C'est une plateforme qui offre gratuitement de la m√©moire GPU indispensable au chargement de notre LLM (llama2 13B Chat). Cependant cette m√©moire est assez limit√©e si on veut charger des mod√®les avec de plus grands param√®tres. Fort heuresement, Hugging Face a d√©velopp√© des mod√®les de quantification avec la biblioth√®que Bitsandbytes.La quantification va ainsi am√©liorer les performances en r√©duisant les besoins en bande passante m√©moire GPU et en augmentant l'utilisation du cache.
