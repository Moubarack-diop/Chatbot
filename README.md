# Chatbot

## AperÃ§u du projet
Notre projet consiste Ã  concevoir un chatbot destinÃ© Ã  assister le service client du Groupe Orange SÃ©nÃ©gal. Ce chatbot est basÃ© sur l'Intelligence Artificielle et utilise dans sa gÃ©nÃ©ration de rÃ©ponse, un grand modÃ¨le de langage naturel (LLM). Dans notre cas nous avons utilisÃ© le Llama2 13B Chat, un modÃ¨le de langage dÃ©veloppÃ© par Meta AI, disponible via l'interface de Hugging Face Transformers.
Cependant,bien que les LLM soient puissants et capables de gÃ©nÃ©rer du contenu crÃ©atif, ils peuvent produire des informations obsolÃ¨tes ou incorrectes car ils sont formÃ©s sur des donnÃ©es statiques. Pour surmonter cette limitation, les systÃ¨mes de gÃ©nÃ©ration augmentÃ©e de rÃ©cupÃ©ration (RAG) peuvent Ãªtre utilisÃ©s pour connecter notre LLM Ã  des donnÃ©es externes et obtenir des rÃ©ponses plus fiables. 
Nous avons alors rÃ©cupÃ©rÃ© les donnÃ©es FAQs contenues dans le site de [Orange Assistance](https://assistance.orange.sn/) grace au web scraping qu'on a directement ingÃ©rÃ© dans le LLM grace au RAG.
## C'est quoi le RAG

Les LLM sont formÃ©s sur un corpus de donnÃ©es volumineux mais fixe, ce qui limite leur capacitÃ© Ã  raisonner sur des informations privÃ©es ou rÃ©centes. Le fine tuning est un moyen d'attÃ©nuer ce problÃ¨me, mais il n'est souvent pas bien adaptÃ© au rappel factuel et peut Ãªtre coÃ»teux. La gÃ©nÃ©ration augmentÃ©e de rÃ©cupÃ©ration (RAG) est devenue un mÃ©canisme populaire et puissant pour Ã©largir la base de connaissances d'un LLM, en utilisant des documents rÃ©cupÃ©rÃ©s Ã  partir d'une source de donnÃ©es externe pour fonder la gÃ©nÃ©ration de LLM via l'injection de contexte.

![Workflow_rag (1)](https://github.com/user-attachments/assets/d8179e97-5b5f-4c6c-9688-0aee3b5eff3a)

L'architecture ci-dessus est une excellente introduction aux bases de l'injection de contexte. Nous allons rÃ©sumer le processus d'injection de contexte comme suit :
1. D'abord nous collectons d'abord les donnÃ©es extraites du site de Orange Assistance grace au web scraping. Ces derniÃ¨res sont ensuite transformÃ©es en fichier CSV.
   
2. Ensuite, les donnÃ©es sont chargÃ©es (text loader) puis segmentÃ©es (text splitting). Les segments (tokens) sont essentiellement une courte chaÃ®ne de caractÃ¨res, gÃ©nÃ©ralement de 4 caractÃ¨res. Par exemple, le mot Â« gÃ©nÃ©ratif Â» peut Ãªtre divisÃ© en segments comme Â« ge Â», Â« n Â», Â« erat Â» et Â« ive Â». Le LLM traite les donnÃ©es sous forme de segments.
 
3. Les tokens seront introduits dans un modÃ¨le d'embedding qui convertit les tokens en vecteurs. L'embeddings sont la faÃ§on dont les mots et les phrases sont reprÃ©sentÃ©s dans un espace vectoriel.Il existe plusieurs modÃ¨les d'embedding dans le marchÃ© dans notre cas nous avons choisi celui de Hugging Face.
  
4. Les vecteurs produits Ã  partir du modÃ¨le d'embedding sont stockÃ©s dans des bases de donnÃ©es vectorielles. Dans notre cas, nous utiliserons ChromaDB.
   
5. Passons maintenant Ã  la partie intÃ©ressante. Lorsqu'un utilisateur pose une question, nous convertissons sa requÃªte en vecteur et recherchons les vecteurs les plus proches dans la base de donnÃ©es. Essentiellement, ce processus localise les fragments de texte les plus pertinents pour la requÃªte de l'utilisateur et les reconvertit en texte.
   
6. La question de l'utilisateur et les fragments de texte pertinents (contexte) seront inclus dans un prompt qui sera fourni au LLM. Sans aucune modification du LLM d'origine, le modÃ¨le peut fournir des rÃ©ponses impressionnantes Ã  la requÃªte en utilisant le contexte injectÃ©.

7. La rÃ©ponse gÃ©nÃ©rÃ©e et la question prÃ©cÃ©dente de l'utilisateur seront introduites dans le prompt pour servir d'historique des conversations. Le LLM pourra s'en inspirÃ© lorsque l'utilisateur pose une autre question en rapport avec les question et rÃ©ponse prÃ©cÃ©dentes.

## Technologies utilisÃ©es

### ðŸ¦œï¸ðŸ”— Langchain
LangChain est un framework open-source conÃ§u pour faciliter le dÃ©veloppement d'applications basÃ©es sur des modÃ¨les de langage (LLMs). Il fournit des ouils pour les pipelines de traitement de langage, gÃ¨re les conversations longues en gÃ©rant l'historique contextuel. Il facilite l'intÃ©gration des bases de connaissances de meme que la conception des prompts.

### ðŸ¤— Hugging Face
Hugging Face est une plateforme qui offre la posibilitÃ© d'accÃ©der Ã  des modÃ¨les de langage prÃ©entrainÃ©s Open source. Pour le cadre de notre projet, nous y avons tÃ©lÃ©chargÃ© le modÃ¨le Llama-2-13b-chat-hf. C'est un modÃ¨le gratuit fourni par Meta AI. NÃ©anmoins il existe d'autres LLM performants sur le marchÃ© tels que GPT, Gemini,...; leur utilisation Ã©tant payante, nous avons prÃ©fÃ©rÃ© travailler avec des LLM gratuits.

### ChromaDB
ChromaDB est une base de donnÃ©es vectorielle spÃ©cialement conÃ§ue pour stocker et rechercher des donnÃ©es vectorielles. Elle va nous permettre de gÃ©rer nos donnÃ©es vectorisÃ©es Ces vecteurs seront utilisÃ©s pour effectuer des recherches basÃ©es sur la similaritÃ©.

## Installation
Pour le dÃ©veloppement de notre Chatbot nous avons utilisÃ© Google Colab. C'est une plateforme qui offre gratuitement de la mÃ©moire GPU indispensable au chargement de notre LLM (llama2 13B Chat). Cependant cette mÃ©moire est assez limitÃ©e si on veut charger des modÃ¨les avec de plus grands paramÃ¨tres. Fort heuresement, Hugging Face a dÃ©veloppÃ© des modÃ¨les de quantification avec la bibliothÃ¨que Bitsandbytes.La quantification va ainsi amÃ©liorer les performances en rÃ©duisant les besoins en bande passante mÃ©moire GPU et en augmentant l'utilisation du cache.
>bzfzbc
>q,dkcn
