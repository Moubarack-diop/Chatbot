{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moubarack-diop/Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation des donn√©es"
      ],
      "metadata": {
        "id": "RyJNbsN7Iw10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette √©tape, nous avons eu √† importer le fichier CSV qui contient les donn√©es FAQs qu'on a extraites du site de Orange Assistance https://assistance.orange.sn/. Cette extraction s'est faite via du Web Scraping en utilisant Beautiful Soup.\n",
        "Le fichier est constitu√© de deux colonnes: question et answer\n"
      ],
      "metadata": {
        "id": "24lKRL58JavH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXRlTejvczeA"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Importer le fichier\n",
        "uploaded = files.upload()\n",
        "# Lire le fichier CSV en tant que DataFrame\n",
        "for filename in uploaded.keys():\n",
        "    df = pd.read_csv(filename)\n",
        "    print(f\"Contenu de {filename} :\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation des biblioth√®ques"
      ],
      "metadata": {
        "id": "J_YO63AlKdQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdvBKx_Nc9M3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb transformers sentence-transformers bitsandbytes\n",
        "!pip install -U langchain-community\n",
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install accelerate\n",
        "!pip install python-telegram-bot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain** : C'est une biblioth√®que Python utilis√©e pour construire des cha√Ænes (chains) d'interactions avec des mod√®les de langage comme les LLM. Elle est particuli√®rement utile pour int√©grer des bases de donn√©es et g√©rer les prompts.\n",
        "\n",
        "**chromadb**: C'est une base de donn√©es vectorielle l√©g√®re. Elle est utilis√©e pour stocker et rechercher des vecteurs (comme ceux g√©n√©r√©s √† partir de texte), souvent dans des syst√®mes de r√©cup√©ration augment√©e (RAG).\n",
        "\n",
        "**transformers** :C'est une biblioth√®que de Hugging Face pour utiliser des mod√®les de machine learning, comme GPT, BERT, ou Llama, dans des t√¢ches de traitement du langage naturel.\n",
        "\n",
        "**sentence-transformers** : Sp√©cialis√©e dans la cr√©ation d'encodages vectoriels pour des textes, elle est utilis√©e pour des t√¢ches comme la recherche s√©mantique ou la d√©tection de similarit√©s.\n",
        "\n",
        "**bitsandbytes** : Une biblioth√®que optimis√©e pour les calculs de faible pr√©cision sur GPU, permettant d'ex√©cuter de gros mod√®les LLM tout en consommant moins de m√©moire.\n",
        "\n",
        "**-U** : Cette option permet de mettre √† jour le paquet √† la derni√®re version disponible.\n",
        "\n",
        "**langchain-community** : Fournit des int√©grations, outils et fonctionnalit√©s suppl√©mentaires d√©velopp√©s par la communaut√© autour de LangChain.\n",
        "\n",
        "**torch** : Une biblioth√®que de deep learning, tr√®s utilis√©e pour entra√Æner et ex√©cuter des mod√®les de machine learning. De nombreux mod√®les LLM reposent sur PyTorch.\n",
        "\n",
        "**accelerate** : Une biblioth√®que de Hugging Face qui permet de g√©rer efficacement l'entra√Ænement et l'inf√©rence sur GPU ou CPU, notamment dans des environnements distribu√©s.\n",
        "\n",
        "**python-telegram-bot** : Une biblioth√®que pour cr√©er et g√©rer des bots Telegram en Python. Elle eprmet l'int√©gration de chatbot sur T√©l√©gram"
      ],
      "metadata": {
        "id": "Pi9iRkawLKTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW4P6zMddFH8"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from torch import cuda, bfloat16\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from huggingface_hub import login\n",
        "from langchain.document_loaders.csv_loader import CSVLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration de la quantification de la m√©moire"
      ],
      "metadata": {
        "id": "hNuqv8vMPzAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration pour la quantification du mod√®le\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# D√©tecter si un GPU est disponible\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "p6lASuV1HDhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'impl√©mentation d'un chatbot utilisant un LLM, l'int√©gration de la quantification de la m√©moire est tr√®s importante. Les mod√®les de LLM sont tr√®s volumineux, et cela consomme beaucoup de m√©moire GPU lors de leur chargement sur Google Colab. Afin d'optimiser notre m√©moire GPU nous avons introduit la biblioth√®que BitsAndBytes"
      ],
      "metadata": {
        "id": "9n0L6G0wP_QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connexion √† Hugging Face"
      ],
      "metadata": {
        "id": "VuPAjVa-SAuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBINDY0B3e9o"
      },
      "outputs": [],
      "source": [
        "# Connexion au Hugging Face Hub\n",
        "login(\"HugginFace_Token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des donn√©es"
      ],
      "metadata": {
        "id": "Dgjz5oe8win2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette √©tape consiste √† charger nos donn√©es contenues dans le fichier final_data.csv et de les transformer en DataFrame.\n",
        "**Document**, qui est une classe Langchain va nous permettre ici de bien formater les documents charger.Cela facilite la recherche, l'interrogation des donn√©es."
      ],
      "metadata": {
        "id": "Nsk012Otw75d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du fichier CSV\n",
        "df = pd.read_csv('final_data.csv')"
      ],
      "metadata": {
        "id": "FWoLbrh6HgYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©er des documents √† partir des questions et r√©ponses\n",
        "documents = [Document(page_content=row['question'], metadata={\"answer\": row['answer']}) for index, row in df.iterrows()]"
      ],
      "metadata": {
        "id": "ue5ijFN7HkQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Splitting\n"
      ],
      "metadata": {
        "id": "fhcsNtGLzsiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le text splitting consiste √† diviser les textes contenus dans le document charg√© en de plus petits √©l√®ments appel√© chunks."
      ],
      "metadata": {
        "id": "c2uYRb9WzwQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# D√©couper les documents en chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "Z342biSHHkNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RecursiveCharacterTextSplitter**: C'est une classe de LangChainutiliser pour d√©couper des textes de mani√®re intelligente.\n",
        "Contrairement √† un d√©coupage strict bas√© sur une taille fixe, il divise le texte en respectant les limites naturelles comme les mots, les phrases ou les paragraphes pour √©viter de couper au milieu d'une id√©e ou d'une phrase.\n",
        "\n",
        "**chunk_size=1000** : Cela sp√©cifie que chaque segment (chunk) doit contenir au maximum 1 000 caract√®res.\n",
        "C'est utile pour respecter les limites des mod√®les ou faciliter la manipulation des segments.\n",
        "\n",
        "**chunk_overlap=20** : Indique que chaque segment peut chevaucher le pr√©c√©dent sur 20 caract√®res.\n",
        "Ce chevauchement garantit que les informations importantes √† la fronti√®re entre deux segments ne sont pas perdues, am√©liorant la continuit√© contextuelle.\n",
        "\n",
        "**split_documents(documents)**: documents est une liste d'objets contenant les donn√©es textuelles √† d√©couper.\n",
        "Chaque document de la liste est d√©coup√© en plusieurs segments (chunks) selon les r√®gles d√©finies par chunk_size et chunk_overlap.\n",
        "\n",
        "**all_splits**: C'est une liste qui contient tous les segments g√©n√©r√©s apr√®s le d√©coupage des documents.\n"
      ],
      "metadata": {
        "id": "O5_-bjaT14ZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "1VyjT7xl201S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apr√®s avoir charg√© et d√©coup√© les documents, les chunks seront transform√©s en donn√©es vectorielles. Pour cela nous avons utilis√©\n",
        " le mod√®le d'embedding de Hugging Face  **sentence-transformers/all-mpnet-base-v2**"
      ],
      "metadata": {
        "id": "Vm3oNkm123Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le mod√®le d'embeddings\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"device\": device})"
      ],
      "metadata": {
        "id": "-xBV24QvHkKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cr√©ation de la base de donn√©es vectorielles avec ChromaDB\n",
        "\n"
      ],
      "metadata": {
        "id": "VH4PlLzg4Nym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChromaDB est un Vector Store qui va nous permettre de stocker les donn√©es vectorielles (data embedding)\n"
      ],
      "metadata": {
        "id": "BPu4nL-x4ZZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©er une base de donn√©es vectorielle avec Chroma\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ],
      "metadata": {
        "id": "ANocUPfEHkHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du mod√®le et du tokeniser"
      ],
      "metadata": {
        "id": "N5bxZd7550Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons maintenant charger notre mod√®le de langage pr√©entrain√©,ainsi que son tokeniser.\n",
        "\n",
        "**model_id**: est une cha√Æne qui sp√©cifie l'identifiant du mod√®le √† charger.Dans notre cas, nous avons utilis√© Llama 2 13B Chat, un mod√®le de langage d√©velopp√© par Meta AI, disponible via l'interface de Hugging Face Transformers.\n",
        "\n",
        "**AutoModelForCausalLM** : Cette classe est utilis√©e pour charger des mod√®les con√ßus pour des t√¢ches de g√©n√©ration de texte (causal language modeling). Les mod√®les causaux pr√©disent le mot suivant dans une s√©quence donn√©e.\n",
        "\n",
        "**from_pretrained** : Cette m√©thode t√©l√©charge et charge le mod√®le pr√©-entra√Æn√© sp√©cifi√© par model_id. Cela inclut le t√©l√©chargement des poids du mod√®le depuis les serveurs de Hugging Face.\n",
        "\n",
        "**quantization_config** : (optionnel) D√©finit la configuration de quantification,afin d'optimiser la m√©moire et la vitesse d'ex√©cution via bnb_config.\n",
        "\n",
        "**torch_dtype** : Sp√©cifie le type de donn√©es pour les poids du mod√®le.\n",
        "torch.float16 (16 bits) est utilis√© pour ex√©cuter le mod√®le en virgule flottante 16 bits, r√©duisant ainsi les besoins en m√©moire et acc√©l√©rant l'ex√©cution sur les GPU.\n",
        "\n",
        "**torch.float32 (32 bits)** est utilis√© comme solution de secours sur les CPU, car ils ne supportent g√©n√©ralement pas la virgule flottante 16 bits.\n",
        "\n",
        "**device** : V√©rifie si CUDA (GPU) est disponible pour effectuer le calcul. Si device == \"cuda\", la pr√©cision de 16 bits est utilis√©e.\n",
        "\n",
        "**AutoTokenizer** : C'est une classe g√©n√©rique qui charge le tokenizer correspondant au mod√®le sp√©cifi√©.\n",
        "\n",
        "**Le tokenizer** est responsable de convertir du texte brut en tokens (unit√©s compr√©hensibles par le mod√®le) et de reconstruire le texte √† partir des pr√©dictions du mod√®le.\n"
      ],
      "metadata": {
        "id": "qeT_Ycmi56s8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0R-ZO8R3nCe"
      },
      "outputs": [],
      "source": [
        "# Charger le mod√®le et tokenizer\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation de la pipeline"
      ],
      "metadata": {
        "id": "ztfppTuZ6PvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©er le pipeline de g√©n√©ration\n",
        "query_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n"
      ],
      "metadata": {
        "id": "r4zZottKH0_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On configure un pipeline de g√©n√©ration de texte en utilisant la biblioth√®que Hugging Face Transformers. Le pipeline combine notre mod√®le `Llama` pr√©-entra√Æn√© (`model`) et son tokenizer (`tokenizer`), qui encode et d√©code les donn√©es textuelles. Il optimise √©galement les performances en d√©finissant le type de donn√©es √† utiliser : **`float16`** pour des calculs rapides et √©conomes en m√©moire sur GPU, ou **`float32`** pour des calculs pr√©cis sur CPU. Une fois configur√©, ce pipeline sera appel√© dans la suite pour produire du texte g√©n√©r√© en fonction des requ√™tes ou des prompts sp√©cifiques."
      ],
      "metadata": {
        "id": "ncTm2UFRSido"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRYza3-S3Ba2"
      },
      "outputs": [],
      "source": [
        "# Initialiser la m√©moire\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    k=5,  # Garder la derni√®re conversation\n",
        "    memory_key=\"chat_history\",\n",
        "    input_key=\"question\",\n",
        "    output_key=\"answer\",\n",
        "    return_messages=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On initialise un syst√®me de m√©moire conversationnelle en utilisant une classe appel√©e **`ConversationBufferWindowMemory`**, qui est con√ßue pour conserver un historique des interactions entre l'utilisateur et le Chatbot. La m√©moire est configur√©e pour stocker uniquement les **5 derni√®res interactions** (gr√¢ce au param√®tre `k=5`), ce qui permet de maintenir un historique limit√© et pertinent pour les r√©ponses contextuelles. Elle associe les cl√©s d'entr√©e et de sortie : la **question** (`input_key=\"question\"`) et la **r√©ponse** (`output_key=\"answer\"`), facilitant ainsi le suivi des √©changes. En activant **`return_messages=True`**, les messages sont retourn√©s sous une forme structur√©e, rendant l'historique plus accessible pour une utilisation dans la suite du programme."
      ],
      "metadata": {
        "id": "phSZCzdVT9eP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dB22RMFdZDi"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query):\n",
        "    \"\"\"\n",
        "    G√©n√®re une r√©ponse √† la question de l'utilisateur en tenant compte de l'historique\n",
        "    \"\"\"\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "        # R√©cup√©rer des documents pertinents\n",
        "        docs = vectordb.similarity_search(query)\n",
        "\n",
        "        # R√©cup√©rer l'historique de la conversation\n",
        "        chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
        "\n",
        "        # Formater l'historique des conversations\n",
        "        formatted_history = \"\"\n",
        "        if chat_history:\n",
        "            formatted_history = \"\\n\".join([\n",
        "                f\"{'Utilisateur' if message.type == 'human' else 'Assistant'}: {message.content}\"\n",
        "                for message in chat_history\n",
        "            ])\n",
        "\n",
        "        # V√©rifier si des documents ont √©t√© trouv√©s\n",
        "        if docs:\n",
        "            all_retrieved_answers = \"\\n\".join([doc.metadata['answer'] for doc in docs])\n",
        "        else:\n",
        "            return \"Je suis d√©sol√©, je n'ai pas assez d'informations pour r√©pondre √† cette question.\"\n",
        "\n",
        "        # Pr√©parer le contexte complet avec l'historique\n",
        "        full_context = f\"\"\"\n",
        "Tu es Tontoo, une Intelligence Artificielle d√©di√©e √† fournir des r√©ponses sur Orange S√©n√©gal.\n",
        "Ton r√¥le est de transmettre des informations utiles et pertinentes en te basant strictement sur le contexte fourni.\n",
        "\n",
        "Directives :\n",
        "\n",
        "1. R√©ponse bas√©e sur le contexte fourni :\n",
        "   - Utilise exclusivement les informations dans le contexte donn√© pour r√©pondre aux questions.\n",
        "   - Si une information est absente du contexte ou si tu n‚Äôas pas la r√©ponse, dis : ¬´ Je n'ai pas assez d'informations. Consulte www.orange.sn pour plus de d√©tails. ¬ª\n",
        "   - Inclus imp√©rativement tous les liens disponibles dans le contexte.\n",
        "\n",
        "2. Gestion du contexte :\n",
        "   - Si le contexte est vide, r√©ponds simplement : ¬´ Je n'ai aucune information. Consulte www.orange.sn pour plus de d√©tails. ¬ª\n",
        "   - Ne fais jamais d‚Äôhypoth√®ses ou de suppositions en l'absence de contexte.\n",
        "\n",
        "3. Traitement des questions et du langage :\n",
        "   - Si la question contient des propos inappropri√©s ou offensants, r√©ponds : ¬´ Je ne r√©ponds pas √† ce type de langage. ¬ª\n",
        "   - Si la demande est vague ou ambigu√´, invite l‚Äôutilisateur √† pr√©ciser sa question pour mieux r√©pondre.\n",
        "\n",
        "4. Style de r√©ponse :\n",
        "   - Ne r√©ponds √† aucune question qui n'est pas en rapport avec Orange S√©n√©gal.\n",
        "   - R√©ponds de mani√®re concise, professionnelle et amicale.\n",
        "   - √âvite de montrer les documents de r√©f√©rence ; concentre-toi uniquement sur les r√©ponses claires et directes.\n",
        "\n",
        "5. Exactitude et transparence :\n",
        "   - Ne fais pas de sp√©culations et sois pr√©cis dans tes r√©ponses.\n",
        "   - Se limiter imperativement a r√©pondre a la question pos√©e\n",
        "   - Si une information te semble incertaine ou ambigu√´, indique-le clairement.\n",
        "   - √©vite au maximum les r√©p√©titions dans tes r√©ponses\n",
        "\n",
        "6. Sp√©cificit√© √† Orange S√©n√©gal :\n",
        "   - Mets en avant les offres, produits et services sp√©cifiques √† Orange S√©n√©gal.\n",
        "   - Rappelle aux utilisateurs de visiter www.orange.sn pour obtenir des informations compl√®tes et actualis√©es.\n",
        "\n",
        "\n",
        "Documents pertinents trouv√©s:\n",
        "{all_retrieved_answers}\n",
        "\n",
        "Historique des conversations:\n",
        "{formatted_history}\n",
        "\n",
        "Question actuelle: {query}\n",
        "R√©ponse:\"\"\"\n",
        "\n",
        "        # G√©n√©rer la r√©ponse\n",
        "        outputs = query_pipeline(\n",
        "            full_context,\n",
        "            max_new_tokens=1500,\n",
        "            clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        response = outputs[0][\"generated_text\"].split(\"R√©ponse:\")[-1].strip()\n",
        "\n",
        "        # Sauvegarder le contexte dans la m√©moire\n",
        "        memory.save_context(\n",
        "            {\"question\": query},\n",
        "            {\"answer\": response}\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"Error: {e}\")\n",
        "        torch.cuda.empty_cache()\n",
        "        return \"D√©sol√©, je n'ai pas pu traiter votre demande.\"\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Efface l'historique de la conversation\"\"\"\n",
        "    memory.clear()\n",
        "\n",
        "def get_conversation_history():\n",
        "    \"\"\"R√©cup√®re l'historique de la conversation\"\"\"\n",
        "    return memory.load_memory_variables({})[\"chat_history\"]\n",
        "\n",
        "def print_conversation_history():\n",
        "    \"\"\"Affiche l'historique de la conversation\"\"\"\n",
        "    history = get_conversation_history()\n",
        "    if not history:\n",
        "        print(\"Aucun historique de conversation disponible.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Historique des conversations ===\")\n",
        "    for message in history:\n",
        "        role = \"Utilisateur\" if message.type == \"human\" else \"Assistant\"\n",
        "        print(f\"{role}: {message.content}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous mettons en place un syst√®me conversationnel. Cela g√©n√®re des r√©ponses pertinentes en utilisant des documents extraits de notre base vectorielle, l‚Äôhistorique des interactions, et des directives sp√©cifiques. La fonction principale, **`generate_answer(query)`**, commence par lib√©rer la m√©moire GPU pour optimiser les performances, puis recherche des documents pertinents li√©s √† la requ√™te de l'utillisateur dans la base de donn√©es vectorielle (**`vectordb.similarity_search`**). Elle r√©cup√®re √©galement l‚Äôhistorique des conversations via la m√©moire (**`memory.load_memory_variables`**) pour fournir des r√©ponses contextualis√©es. Si des documents pertinents sont trouv√©s, ils enrichissent le contexte, sinon un message standard informe l‚Äôutilisateur d‚Äôun manque d‚Äôinformations. Un contexte global est ensuite construit en int√©grant les documents, l‚Äôhistorique format√©, et des instructions strictes pour limiter les r√©ponses au cadre d‚Äô**Orange S√©n√©gal**. Ce contexte est trait√© par un pipeline de g√©n√©ration de texte (**`query_pipeline`**), qui produit une r√©ponse nettoy√©e et concise, sauvegard√©e ensuite dans la m√©moire pour des interactions ult√©rieures. Des fonctions auxiliaires, comme **`clear_memory`**, **`get_conversation_history`**, et **`print_conversation_history`**, permettent de g√©rer et d‚Äôafficher l‚Äôhistorique des conversations."
      ],
      "metadata": {
        "id": "zA5hztVwU6Sf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIjBVcO-uqqV"
      },
      "source": [
        "Historique des conversations:\n",
        "{formatted_history}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfHbgPlYdEA_"
      },
      "outputs": [],
      "source": [
        "from telegram import Update, Bot\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Initialiser nest_asyncio pour Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configuration du token Telegram\n",
        "TELEGRAM_TOKEN = 'Telegram_token'\n",
        "\n",
        "# Fonction pour d√©marrer le bot avec un message d'accueil personnalis√© pour Orange S√©n√©gal\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    # Message d'accueil\n",
        "    message = (\n",
        "        \"<b>Bonjour et bienvenue sur Tontoo, le Chatbot d'Orange S√©n√©gal !</b>\\n\\n\"\n",
        "        \"Ce chatbot est con√ßu pour vous offrir une assistance en temps r√©el. Posez vos questions et nous vous fournirons des r√©ponses \"\n",
        "        \"rapides et pr√©cises. Il comprend et r√©pond en francais.\\n\\n\"\n",
        "        \"Voici quelques commandes pour vous aider √† naviguer :\\n\"\n",
        "        \"‚Ä¢ /start - D√©marrer le bot\\n\"\n",
        "        \"‚Ä¢ /clear - Effacer l'historique\\n\"\n",
        "        \"‚Ä¢ /historique - Afficher l'historique de la conversation\\n\\n\"\n",
        "        \"Nous sommes l√† pour r√©pondre √† toutes vos questions. N'h√©sitez pas √† interagir avec nous ! üòä\"\n",
        "    )\n",
        "    # Envoyer le message\n",
        "    await update.message.reply_text(message, parse_mode=\"HTML\")\n",
        "\n",
        "\n",
        "# Fonction pour traiter les messages utilisateur\n",
        "async def answer(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    user_query = update.message.text\n",
        "    response = generate_answer(user_query)\n",
        "    torch.cuda.empty_cache()\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "# Fonction pour effacer la m√©moire de conversation\n",
        "async def clear(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    clear_memory()  # Appel de la fonction clear_memory pour effacer l'historique\n",
        "    await update.message.reply_text(\"L'historique de la conversation a √©t√© effac√©.\")\n",
        "\n",
        "# Fonction pour afficher la m√©moire de conversation\n",
        "async def historique(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    history = get_conversation_history()\n",
        "    if not history:\n",
        "        await update.message.reply_text(\"Aucun historique de conversation disponible.\")\n",
        "    else:\n",
        "        formatted_history = \"\\n\".join(\n",
        "            f\"{'Utilisateur' if message.type == 'human' else 'Assistant'}: {message.content}\"\n",
        "            for message in history\n",
        "        )\n",
        "        await update.message.reply_text(f\"Historique des conversations :\\n{formatted_history}\")\n",
        "\n",
        "# Cr√©er une application Telegram\n",
        "app = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "\n",
        "# Ajouter les handlers pour les commandes et les messages\n",
        "app.add_handler(CommandHandler(\"start\", start))\n",
        "app.add_handler(CommandHandler(\"clear\", clear))\n",
        "app.add_handler(CommandHandler(\"historique\", historique))\n",
        "app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, answer))\n",
        "\n",
        "# D√©marrer le bot en mode polling\n",
        "print(\"Bot en ligne sur Telegram.\")\n",
        "app.run_polling()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On int√®gre le bot sur Telegram en utilisant les biblioth√®ques **`telegram`** et **`telegram.ext`**. Le bot est configur√© avec un **token Telegram** unique pour se connecter √† l'application et fonctionne dans un environnement asynchrone comme Google Colab gr√¢ce √† **`nest_asyncio`**. Plusieurs fonctionnalit√©s sont propos√©es, notamment une commande **`/start`**, qui affiche un message d'accueil d√©taill√© en HTML, pr√©sentant le chatbot et ses capacit√©s, ainsi que les commandes disponibles. La commande **`/clear`** permet d'effacer l‚Äôhistorique des conversations via la fonction **`clear_memory`**, tandis que **`/historique`** affiche les √©changes pr√©c√©dents sous un format lisible en r√©cup√©rant les messages enregistr√©s dans la m√©moire. Lorsqu‚Äôun utilisateur envoie un message texte, la fonction **`answer`** g√©n√®re une r√©ponse pertinente en appelant la fonction **`generate_answer`**. Les interactions sont g√©r√©es √† l‚Äôaide de *handlers*, qui associent des commandes sp√©cifiques ou des messages texte √† leurs fonctions correspondantes. Enfin, le bot est lanc√© en mode *polling*, ce qui lui permet de surveiller en continu les nouveaux messages pour y r√©pondre."
      ],
      "metadata": {
        "id": "oLZoo5jcZpZL"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
