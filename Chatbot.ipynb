{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moubarack-diop/Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation des données"
      ],
      "metadata": {
        "id": "RyJNbsN7Iw10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette étape, nous avons eu à importer le fichier CSV qui contient les données FAQs qu'on a extraites du site de Orange Assistance https://assistance.orange.sn/. Cette extraction s'est faite via du Web Scraping en utilisant Beautiful Soup.\n",
        "Le fichier est constitué de deux colonnes: question et answer\n"
      ],
      "metadata": {
        "id": "24lKRL58JavH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXRlTejvczeA"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Importer le fichier\n",
        "uploaded = files.upload()\n",
        "# Lire le fichier CSV en tant que DataFrame\n",
        "for filename in uploaded.keys():\n",
        "    df = pd.read_csv(filename)\n",
        "    print(f\"Contenu de {filename} :\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation des bibliothèques"
      ],
      "metadata": {
        "id": "J_YO63AlKdQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdvBKx_Nc9M3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb transformers sentence-transformers bitsandbytes\n",
        "!pip install -U langchain-community\n",
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install accelerate\n",
        "!pip install python-telegram-bot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain** : C'est une bibliothèque Python utilisée pour construire des chaînes (chains) d'interactions avec des modèles de langage comme les LLM. Elle est particulièrement utile pour intégrer des bases de données et gérer les prompts.\n",
        "\n",
        "**chromadb**: C'est une base de données vectorielle légère. Elle est utilisée pour stocker et rechercher des vecteurs (comme ceux générés à partir de texte), souvent dans des systèmes de récupération augmentée (RAG).\n",
        "\n",
        "**transformers** :C'est une bibliothèque de Hugging Face pour utiliser des modèles de machine learning, comme GPT, BERT, ou Llama, dans des tâches de traitement du langage naturel.\n",
        "\n",
        "**sentence-transformers** : Spécialisée dans la création d'encodages vectoriels pour des textes, elle est utilisée pour des tâches comme la recherche sémantique ou la détection de similarités.\n",
        "\n",
        "**bitsandbytes** : Une bibliothèque optimisée pour les calculs de faible précision sur GPU, permettant d'exécuter de gros modèles LLM tout en consommant moins de mémoire.\n",
        "\n",
        "**-U** : Cette option permet de mettre à jour le paquet à la dernière version disponible.\n",
        "\n",
        "**langchain-community** : Fournit des intégrations, outils et fonctionnalités supplémentaires développés par la communauté autour de LangChain.\n",
        "\n",
        "**torch** : Une bibliothèque de deep learning, très utilisée pour entraîner et exécuter des modèles de machine learning. De nombreux modèles LLM reposent sur PyTorch.\n",
        "\n",
        "**accelerate** : Une bibliothèque de Hugging Face qui permet de gérer efficacement l'entraînement et l'inférence sur GPU ou CPU, notamment dans des environnements distribués.\n",
        "\n",
        "**python-telegram-bot** : Une bibliothèque pour créer et gérer des bots Telegram en Python. Elle eprmet l'intégration de chatbot sur Télégram"
      ],
      "metadata": {
        "id": "Pi9iRkawLKTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW4P6zMddFH8"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from torch import cuda, bfloat16\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from huggingface_hub import login\n",
        "from langchain.document_loaders.csv_loader import CSVLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration de la quantification de la mémoire"
      ],
      "metadata": {
        "id": "hNuqv8vMPzAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration pour la quantification du modèle\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# Détecter si un GPU est disponible\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "p6lASuV1HDhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'implémentation d'un chatbot utilisant un LLM, l'intégration de la quantification de la mémoire est très importante. Les modèles de LLM sont très volumineux, et cela consomme beaucoup de mémoire GPU lors de leur chargement sur Google Colab. Afin d'optimiser notre mémoire GPU nous avons introduit la bibliothèque BitsAndBytes"
      ],
      "metadata": {
        "id": "9n0L6G0wP_QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connexion à Hugging Face"
      ],
      "metadata": {
        "id": "VuPAjVa-SAuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBINDY0B3e9o"
      },
      "outputs": [],
      "source": [
        "# Connexion au Hugging Face Hub\n",
        "login(\"HugginFace_Token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des données"
      ],
      "metadata": {
        "id": "Dgjz5oe8win2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette étape consiste à charger nos données contenues dans le fichier final_data.csv et de les transformer en DataFrame.\n",
        "**Document**, qui est une classe Langchain va nous permettre ici de bien formater les documents charger.Cela facilite la recherche, l'interrogation des données."
      ],
      "metadata": {
        "id": "Nsk012Otw75d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du fichier CSV\n",
        "df = pd.read_csv('final_data.csv')"
      ],
      "metadata": {
        "id": "FWoLbrh6HgYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer des documents à partir des questions et réponses\n",
        "documents = [Document(page_content=row['question'], metadata={\"answer\": row['answer']}) for index, row in df.iterrows()]"
      ],
      "metadata": {
        "id": "ue5ijFN7HkQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Splitting\n"
      ],
      "metadata": {
        "id": "fhcsNtGLzsiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le text splitting consiste à diviser les textes contenus dans le document chargé en de plus petits élèments appelé chunks."
      ],
      "metadata": {
        "id": "c2uYRb9WzwQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Découper les documents en chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "Z342biSHHkNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RecursiveCharacterTextSplitter**: C'est une classe de LangChainutiliser pour découper des textes de manière intelligente.\n",
        "Contrairement à un découpage strict basé sur une taille fixe, il divise le texte en respectant les limites naturelles comme les mots, les phrases ou les paragraphes pour éviter de couper au milieu d'une idée ou d'une phrase.\n",
        "\n",
        "**chunk_size=1000** : Cela spécifie que chaque segment (chunk) doit contenir au maximum 1 000 caractères.\n",
        "C'est utile pour respecter les limites des modèles ou faciliter la manipulation des segments.\n",
        "\n",
        "**chunk_overlap=20** : Indique que chaque segment peut chevaucher le précédent sur 20 caractères.\n",
        "Ce chevauchement garantit que les informations importantes à la frontière entre deux segments ne sont pas perdues, améliorant la continuité contextuelle.\n",
        "\n",
        "**split_documents(documents)**: documents est une liste d'objets contenant les données textuelles à découper.\n",
        "Chaque document de la liste est découpé en plusieurs segments (chunks) selon les règles définies par chunk_size et chunk_overlap.\n",
        "\n",
        "**all_splits**: C'est une liste qui contient tous les segments générés après le découpage des documents.\n"
      ],
      "metadata": {
        "id": "O5_-bjaT14ZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "1VyjT7xl201S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après avoir chargé et découpé les documents, les chunks seront transformés en données vectorielles. Pour cela nous avons utilisé\n",
        " le modèle d'embedding de Hugging Face  **sentence-transformers/all-mpnet-base-v2**"
      ],
      "metadata": {
        "id": "Vm3oNkm123Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modèle d'embeddings\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"device\": device})"
      ],
      "metadata": {
        "id": "-xBV24QvHkKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création de la base de données vectorielles avec ChromaDB\n",
        "\n"
      ],
      "metadata": {
        "id": "VH4PlLzg4Nym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChromaDB est un Vector Store qui va nous permettre de stocker les données vectorielles (data embedding)\n"
      ],
      "metadata": {
        "id": "BPu4nL-x4ZZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer une base de données vectorielle avec Chroma\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ],
      "metadata": {
        "id": "ANocUPfEHkHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du modèle et du tokeniser"
      ],
      "metadata": {
        "id": "N5bxZd7550Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons maintenant charger notre modèle de langage préentrainé,ainsi que son tokeniser.\n",
        "\n",
        "**model_id**: est une chaîne qui spécifie l'identifiant du modèle à charger.Dans notre cas, nous avons utilisé Llama 2 13B Chat, un modèle de langage développé par Meta AI, disponible via l'interface de Hugging Face Transformers.\n",
        "\n",
        "**AutoModelForCausalLM** : Cette classe est utilisée pour charger des modèles conçus pour des tâches de génération de texte (causal language modeling). Les modèles causaux prédisent le mot suivant dans une séquence donnée.\n",
        "\n",
        "**from_pretrained** : Cette méthode télécharge et charge le modèle pré-entraîné spécifié par model_id. Cela inclut le téléchargement des poids du modèle depuis les serveurs de Hugging Face.\n",
        "\n",
        "**quantization_config** : (optionnel) Définit la configuration de quantification,afin d'optimiser la mémoire et la vitesse d'exécution via bnb_config.\n",
        "\n",
        "**torch_dtype** : Spécifie le type de données pour les poids du modèle.\n",
        "torch.float16 (16 bits) est utilisé pour exécuter le modèle en virgule flottante 16 bits, réduisant ainsi les besoins en mémoire et accélérant l'exécution sur les GPU.\n",
        "\n",
        "**torch.float32 (32 bits)** est utilisé comme solution de secours sur les CPU, car ils ne supportent généralement pas la virgule flottante 16 bits.\n",
        "\n",
        "**device** : Vérifie si CUDA (GPU) est disponible pour effectuer le calcul. Si device == \"cuda\", la précision de 16 bits est utilisée.\n",
        "\n",
        "**AutoTokenizer** : C'est une classe générique qui charge le tokenizer correspondant au modèle spécifié.\n",
        "\n",
        "**Le tokenizer** est responsable de convertir du texte brut en tokens (unités compréhensibles par le modèle) et de reconstruire le texte à partir des prédictions du modèle.\n"
      ],
      "metadata": {
        "id": "qeT_Ycmi56s8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0R-ZO8R3nCe"
      },
      "outputs": [],
      "source": [
        "# Charger le modèle et tokenizer\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation de la pipeline"
      ],
      "metadata": {
        "id": "ztfppTuZ6PvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer le pipeline de génération\n",
        "query_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n"
      ],
      "metadata": {
        "id": "r4zZottKH0_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On configure un pipeline de génération de texte en utilisant la bibliothèque Hugging Face Transformers. Le pipeline combine notre modèle `Llama` pré-entraîné (`model`) et son tokenizer (`tokenizer`), qui encode et décode les données textuelles. Il optimise également les performances en définissant le type de données à utiliser : **`float16`** pour des calculs rapides et économes en mémoire sur GPU, ou **`float32`** pour des calculs précis sur CPU. Une fois configuré, ce pipeline sera appelé dans la suite pour produire du texte généré en fonction des requêtes ou des prompts spécifiques."
      ],
      "metadata": {
        "id": "ncTm2UFRSido"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRYza3-S3Ba2"
      },
      "outputs": [],
      "source": [
        "# Initialiser la mémoire\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    k=5,  # Garder la dernière conversation\n",
        "    memory_key=\"chat_history\",\n",
        "    input_key=\"question\",\n",
        "    output_key=\"answer\",\n",
        "    return_messages=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On initialise un système de mémoire conversationnelle en utilisant une classe appelée **`ConversationBufferWindowMemory`**, qui est conçue pour conserver un historique des interactions entre l'utilisateur et le Chatbot. La mémoire est configurée pour stocker uniquement les **5 dernières interactions** (grâce au paramètre `k=5`), ce qui permet de maintenir un historique limité et pertinent pour les réponses contextuelles. Elle associe les clés d'entrée et de sortie : la **question** (`input_key=\"question\"`) et la **réponse** (`output_key=\"answer\"`), facilitant ainsi le suivi des échanges. En activant **`return_messages=True`**, les messages sont retournés sous une forme structurée, rendant l'historique plus accessible pour une utilisation dans la suite du programme."
      ],
      "metadata": {
        "id": "phSZCzdVT9eP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dB22RMFdZDi"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query):\n",
        "    \"\"\"\n",
        "    Génère une réponse à la question de l'utilisateur en tenant compte de l'historique\n",
        "    \"\"\"\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "        # Récupérer des documents pertinents\n",
        "        docs = vectordb.similarity_search(query)\n",
        "\n",
        "        # Récupérer l'historique de la conversation\n",
        "        chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
        "\n",
        "        # Formater l'historique des conversations\n",
        "        formatted_history = \"\"\n",
        "        if chat_history:\n",
        "            formatted_history = \"\\n\".join([\n",
        "                f\"{'Utilisateur' if message.type == 'human' else 'Assistant'}: {message.content}\"\n",
        "                for message in chat_history\n",
        "            ])\n",
        "\n",
        "        # Vérifier si des documents ont été trouvés\n",
        "        if docs:\n",
        "            all_retrieved_answers = \"\\n\".join([doc.metadata['answer'] for doc in docs])\n",
        "        else:\n",
        "            return \"Je suis désolé, je n'ai pas assez d'informations pour répondre à cette question.\"\n",
        "\n",
        "        # Préparer le contexte complet avec l'historique\n",
        "        full_context = f\"\"\"\n",
        "Tu es Tontoo, une Intelligence Artificielle dédiée à fournir des réponses sur Orange Sénégal.\n",
        "Ton rôle est de transmettre des informations utiles et pertinentes en te basant strictement sur le contexte fourni.\n",
        "\n",
        "Directives :\n",
        "\n",
        "1. Réponse basée sur le contexte fourni :\n",
        "   - Utilise exclusivement les informations dans le contexte donné pour répondre aux questions.\n",
        "   - Si une information est absente du contexte ou si tu n’as pas la réponse, dis : « Je n'ai pas assez d'informations. Consulte www.orange.sn pour plus de détails. »\n",
        "   - Inclus impérativement tous les liens disponibles dans le contexte.\n",
        "\n",
        "2. Gestion du contexte :\n",
        "   - Si le contexte est vide, réponds simplement : « Je n'ai aucune information. Consulte www.orange.sn pour plus de détails. »\n",
        "   - Ne fais jamais d’hypothèses ou de suppositions en l'absence de contexte.\n",
        "\n",
        "3. Traitement des questions et du langage :\n",
        "   - Si la question contient des propos inappropriés ou offensants, réponds : « Je ne réponds pas à ce type de langage. »\n",
        "   - Si la demande est vague ou ambiguë, invite l’utilisateur à préciser sa question pour mieux répondre.\n",
        "\n",
        "4. Style de réponse :\n",
        "   - Ne réponds à aucune question qui n'est pas en rapport avec Orange Sénégal.\n",
        "   - Réponds de manière concise, professionnelle et amicale.\n",
        "   - Évite de montrer les documents de référence ; concentre-toi uniquement sur les réponses claires et directes.\n",
        "\n",
        "5. Exactitude et transparence :\n",
        "   - Ne fais pas de spéculations et sois précis dans tes réponses.\n",
        "   - Se limiter imperativement a répondre a la question posée\n",
        "   - Si une information te semble incertaine ou ambiguë, indique-le clairement.\n",
        "   - évite au maximum les répétitions dans tes réponses\n",
        "\n",
        "6. Spécificité à Orange Sénégal :\n",
        "   - Mets en avant les offres, produits et services spécifiques à Orange Sénégal.\n",
        "   - Rappelle aux utilisateurs de visiter www.orange.sn pour obtenir des informations complètes et actualisées.\n",
        "\n",
        "\n",
        "Documents pertinents trouvés:\n",
        "{all_retrieved_answers}\n",
        "\n",
        "Historique des conversations:\n",
        "{formatted_history}\n",
        "\n",
        "Question actuelle: {query}\n",
        "Réponse:\"\"\"\n",
        "\n",
        "        # Générer la réponse\n",
        "        outputs = query_pipeline(\n",
        "            full_context,\n",
        "            max_new_tokens=1500,\n",
        "            clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        response = outputs[0][\"generated_text\"].split(\"Réponse:\")[-1].strip()\n",
        "\n",
        "        # Sauvegarder le contexte dans la mémoire\n",
        "        memory.save_context(\n",
        "            {\"question\": query},\n",
        "            {\"answer\": response}\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"Error: {e}\")\n",
        "        torch.cuda.empty_cache()\n",
        "        return \"Désolé, je n'ai pas pu traiter votre demande.\"\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Efface l'historique de la conversation\"\"\"\n",
        "    memory.clear()\n",
        "\n",
        "def get_conversation_history():\n",
        "    \"\"\"Récupère l'historique de la conversation\"\"\"\n",
        "    return memory.load_memory_variables({})[\"chat_history\"]\n",
        "\n",
        "def print_conversation_history():\n",
        "    \"\"\"Affiche l'historique de la conversation\"\"\"\n",
        "    history = get_conversation_history()\n",
        "    if not history:\n",
        "        print(\"Aucun historique de conversation disponible.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== Historique des conversations ===\")\n",
        "    for message in history:\n",
        "        role = \"Utilisateur\" if message.type == \"human\" else \"Assistant\"\n",
        "        print(f\"{role}: {message.content}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous mettons en place un système conversationnel. Cela génère des réponses pertinentes en utilisant des documents extraits de notre base vectorielle, l’historique des interactions, et des directives spécifiques. La fonction principale, **`generate_answer(query)`**, commence par libérer la mémoire GPU pour optimiser les performances, puis recherche des documents pertinents liés à la requête de l'utillisateur dans la base de données vectorielle (**`vectordb.similarity_search`**). Elle récupère également l’historique des conversations via la mémoire (**`memory.load_memory_variables`**) pour fournir des réponses contextualisées. Si des documents pertinents sont trouvés, ils enrichissent le contexte, sinon un message standard informe l’utilisateur d’un manque d’informations. Un contexte global est ensuite construit en intégrant les documents, l’historique formaté, et des instructions strictes pour limiter les réponses au cadre d’**Orange Sénégal**. Ce contexte est traité par un pipeline de génération de texte (**`query_pipeline`**), qui produit une réponse nettoyée et concise, sauvegardée ensuite dans la mémoire pour des interactions ultérieures. Des fonctions auxiliaires, comme **`clear_memory`**, **`get_conversation_history`**, et **`print_conversation_history`**, permettent de gérer et d’afficher l’historique des conversations."
      ],
      "metadata": {
        "id": "zA5hztVwU6Sf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIjBVcO-uqqV"
      },
      "source": [
        "Historique des conversations:\n",
        "{formatted_history}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfHbgPlYdEA_"
      },
      "outputs": [],
      "source": [
        "from telegram import Update, Bot\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Initialiser nest_asyncio pour Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configuration du token Telegram\n",
        "TELEGRAM_TOKEN = 'Telegram_token'\n",
        "\n",
        "# Fonction pour démarrer le bot avec un message d'accueil personnalisé pour Orange Sénégal\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    # Message d'accueil\n",
        "    message = (\n",
        "        \"<b>Bonjour et bienvenue sur Tontoo, le Chatbot d'Orange Sénégal !</b>\\n\\n\"\n",
        "        \"Ce chatbot est conçu pour vous offrir une assistance en temps réel. Posez vos questions et nous vous fournirons des réponses \"\n",
        "        \"rapides et précises. Il comprend et répond en francais.\\n\\n\"\n",
        "        \"Voici quelques commandes pour vous aider à naviguer :\\n\"\n",
        "        \"• /start - Démarrer le bot\\n\"\n",
        "        \"• /clear - Effacer l'historique\\n\"\n",
        "        \"• /historique - Afficher l'historique de la conversation\\n\\n\"\n",
        "        \"Nous sommes là pour répondre à toutes vos questions. N'hésitez pas à interagir avec nous ! 😊\"\n",
        "    )\n",
        "    # Envoyer le message\n",
        "    await update.message.reply_text(message, parse_mode=\"HTML\")\n",
        "\n",
        "\n",
        "# Fonction pour traiter les messages utilisateur\n",
        "async def answer(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    user_query = update.message.text\n",
        "    response = generate_answer(user_query)\n",
        "    torch.cuda.empty_cache()\n",
        "    await update.message.reply_text(response)\n",
        "\n",
        "# Fonction pour effacer la mémoire de conversation\n",
        "async def clear(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    clear_memory()  # Appel de la fonction clear_memory pour effacer l'historique\n",
        "    await update.message.reply_text(\"L'historique de la conversation a été effacé.\")\n",
        "\n",
        "# Fonction pour afficher la mémoire de conversation\n",
        "async def historique(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    history = get_conversation_history()\n",
        "    if not history:\n",
        "        await update.message.reply_text(\"Aucun historique de conversation disponible.\")\n",
        "    else:\n",
        "        formatted_history = \"\\n\".join(\n",
        "            f\"{'Utilisateur' if message.type == 'human' else 'Assistant'}: {message.content}\"\n",
        "            for message in history\n",
        "        )\n",
        "        await update.message.reply_text(f\"Historique des conversations :\\n{formatted_history}\")\n",
        "\n",
        "# Créer une application Telegram\n",
        "app = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "\n",
        "# Ajouter les handlers pour les commandes et les messages\n",
        "app.add_handler(CommandHandler(\"start\", start))\n",
        "app.add_handler(CommandHandler(\"clear\", clear))\n",
        "app.add_handler(CommandHandler(\"historique\", historique))\n",
        "app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, answer))\n",
        "\n",
        "# Démarrer le bot en mode polling\n",
        "print(\"Bot en ligne sur Telegram.\")\n",
        "app.run_polling()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On intègre le bot sur Telegram en utilisant les bibliothèques **`telegram`** et **`telegram.ext`**. Le bot est configuré avec un **token Telegram** unique pour se connecter à l'application et fonctionne dans un environnement asynchrone comme Google Colab grâce à **`nest_asyncio`**. Plusieurs fonctionnalités sont proposées, notamment une commande **`/start`**, qui affiche un message d'accueil détaillé en HTML, présentant le chatbot et ses capacités, ainsi que les commandes disponibles. La commande **`/clear`** permet d'effacer l’historique des conversations via la fonction **`clear_memory`**, tandis que **`/historique`** affiche les échanges précédents sous un format lisible en récupérant les messages enregistrés dans la mémoire. Lorsqu’un utilisateur envoie un message texte, la fonction **`answer`** génère une réponse pertinente en appelant la fonction **`generate_answer`**. Les interactions sont gérées à l’aide de *handlers*, qui associent des commandes spécifiques ou des messages texte à leurs fonctions correspondantes. Enfin, le bot est lancé en mode *polling*, ce qui lui permet de surveiller en continu les nouveaux messages pour y répondre."
      ],
      "metadata": {
        "id": "oLZoo5jcZpZL"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
